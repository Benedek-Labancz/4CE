import os
import argparse
from collections import defaultdict
from tqdm import tqdm

from utils import load_yaml, save_model
from game import _4CE, _2CE
from agent import Agent, ReplayBuffer
from log import record_gym, create_experiment, Logger
from eval import self_play_eval
from config import device

import torch
from torch.optim import Adam
from torch.nn import MSELoss
import numpy as np

import gym


def default_collection(game, logger, buffer, agent, response_func):
    '''
    Play an episode and collect each experience of player X.
    '''
    game.reset()
    done = False
    total_reward = 0
    total_net_reward = 0
    while not done:
        state = game.state
        # Agent's move (X's move)
        action = agent.explore_act(game.flat(state))
        new_state, reward, done, info = game.step(action)
        logger.log("step_reward", reward)
        total_reward += reward

        if not done:
            # Automatic response (O's move, generated by the response agent)
            # this is analogous to being the environment's dynamics.
            response_action = response_func(new_state)
            future_state, opponent_reward, future_done, future_info = game.step(response_action)
            net_reward = reward - opponent_reward
            total_net_reward += net_reward

            # Save the experience for later use
            buffer.extend(game.flat(state), game.coords_to_action(action), net_reward, game.flat(future_state), future_done)
        else:
            buffer.extend(game.flat(state), game.coords_to_action(action), reward, game.flat(new_state), done)
    return game, logger, buffer, total_reward, total_net_reward

def prioritized_collection(game, logger, buffer, agent, spec, response_func):
    '''
    Play an episode and collect only relevant experiences.
    We collect each experience that yields a reward,
    and only collect (100*Z) percent of experiences that yield zero reward.
    '''
    game.reset()
    done = False
    total_reward = 0
    total_net_reward = 0
    while not done:
        state = game.state
        # Agent's move (X's move)
        action = agent.explore_act(game.flat(state))
        new_state, reward, done, info = game.step(action)
        total_reward += reward

        if not done:
            # Automatic response (O's move, generated by the response agent)
            # this is analogous to being the environment's dynamics.
            response_action = response_func(new_state)
            future_state, opponent_reward, future_done, future_info = game.step(response_action)
            net_reward = reward - opponent_reward
            total_net_reward += net_reward
            if net_reward != 0 or np.random.random() < spec["Z"]:
                logger.log("step_reward", net_reward)
                buffer.extend(game.flat(state), game.coords_to_action(action), net_reward, game.flat(future_state), future_done)
        else:
            if reward != 0 or np.random.random() < spec["Z"]:
                logger.log("step_reward", reward)
                buffer.extend(game.flat(state), game.coords_to_action(action), reward, game.flat(new_state), done)
    return game, logger, buffer, total_reward, total_net_reward


def train(game, agent, spec, args):

    print("Training Started.")
    print("Using device", device)

    buffer = ReplayBuffer(max_size=spec["max_buffer_size"])
    optimizer = Adam(params=agent.Q.parameters(), lr=spec["lr"])
    loss_module = MSELoss()

    # We define an agent to simulate environment dynamics. This agent is not trained.
    # Later we will move on to self-play, but we use static dynamics now for simplicity.
    response_agent = Agent(game, spec["num_cells"], spec["epsilon"])

    episode, max_episodes = 0, spec["max_episodes"]
    pbar = tqdm(total=max_episodes)

    if args.log == True:
        _, plot_path, model_path = create_experiment(args.exp_name, args.info, spec=spec)
        logger = Logger(plot_path)
        logger.add_log("epsilon", title="Exploration Term", x_label="Episode", y_label="Epsilon", path="epsilon.png", plot_func=logger.make_line_plot)
        logger.add_log("step_reward", title="Reward distribution", x_label="Reward", y_label="Frequency", path="r_dist.png", plot_func=logger.make_histogram)
        logger.add_log("reward", title="Episode reward", x_label="Total episode reward", y_label="Number of episodes", path="reward.png", plot_func=logger.make_histogram)
        logger.add_log("net_reward", title="Episode net-reward", x_label="Net episode reward", y_label="Number of episodes", path="net_reward.png", plot_func=logger.make_histogram)
        logger.add_log("q", title="Q-value", x_label="Episode", y_label="Average Q-value", path="q.png", plot_func=logger.make_line_plot)
        logger.add_log("loss", title="Loss", x_label="Episode", y_label="Average MSE loss", path="loss.png", plot_func=logger.make_line_plot)
        logger.add_log("grad", title="Gradient of loss w.r.t. Q-network parameters", x_label="Episode", y_label="Average gradient norm", ylim=[0, 1], path="grad.png", plot_func=logger.make_line_plot)

    if args.save == True:
        max_q_value = -np.inf

    '''
    We make a number of random initial steps before starting training.
    '''
    init_episode = 0
    while init_episode < spec["init_random_episodes"]:
        game, logger, buffer, _, _ = prioritized_collection(game, logger, buffer, agent, spec, lambda x: response_agent.act(game.flat(x)))
        init_episode += 1

    print(f"Collected {len(buffer)} experience from {init_episode} episodes.")

    '''
    Main training loop.
    '''
    while episode < max_episodes:
        # Play an episode
        game, logger, buffer, total_reward, total_net_reward = prioritized_collection(game, logger, buffer, agent, spec, lambda x: response_agent.act(game.flat(x)))
        
        '''
        An episode is done, let's improve the agent and perform other operarions.
        '''
        episode += 1
        pbar.update()

        logger.log("reward", total_reward)
        logger.log("net_reward", total_net_reward)
        
        # Epsilon decay; we do it every epsiode for consistency with other operations
        agent.epsilon = max(spec["min_epsilon"], spec["epsilon_decay"] * agent.epsilon)
        logger.log_t("epsilon", episode, agent.epsilon)

        # Sync Target-Network parameters with current Q-network parameters
        if episode % spec["sync_frequency"] == 0:
            agent.sync_target()

        # Perform experience-replay
        if episode % spec["optim_frequency"] == 0:
            q_values, loss_values, grad_norms = [], [], []
            for _ in range(spec["optim_iter"]):
                samples = buffer.sample(spec["batch_size"])
                samples = buffer.batch_samples(samples)

                target = samples["reward"]
                target = target + (1 - samples["done"]) * spec["gamma"] * torch.max(agent.TargetNet(samples["new_state"]))
                target = target.reshape(1, -1)
                current_estimate = torch.gather(agent.Q(samples["state"]), dim=1, index=samples["action"])
                q_values.append(current_estimate.detach().mean())

                loss_value = loss_module(current_estimate, target)
                loss_values.append(loss_value.detach().mean())
                grad_norms.append(torch.autograd.grad(loss_value, agent.Q.parameters(), retain_graph=True)[0].norm())
                loss_value.backward()
                optimizer.step()
                optimizer.zero_grad()

            mean_q = np.array(q_values).mean()
            logger.log_t('q', episode, mean_q)
            logger.log_t('loss', episode, np.array(loss_values).mean())
            logger.log_t('grad', episode, np.array(grad_norms).mean())

            if args.save == True:
                if mean_q > max_q_value:
                    save_model(agent.Q, model_path, 'best')
                    max_q_value = mean_q
                
        if episode % spec["plot_frequency"] == 0:
            logger.plot_all()

if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument('--spec_path', type=str, default="./spec/exp.yml")
    parser.add_argument('--spec_name', type=str)
    parser.add_argument('--exp_name', type=str)
    parser.add_argument('--log', action=argparse.BooleanOptionalAction)
    parser.add_argument('--save', action=argparse.BooleanOptionalAction)
    parser.add_argument('--info', type=str, default="No description provided.")
    args = parser.parse_args()

    spec = load_yaml(args.spec_path, args.spec_name)
    game = _2CE()
    agent = Agent(game, spec["num_cells"], spec["epsilon"])

    agent = train(game, agent, spec, args)

